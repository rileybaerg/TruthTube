{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Sentiment Analysis on Transcript Data\n",
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>id: do8O1YxzdVg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>if you don't like eating veggies this video is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>id: bpsKYwQffes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>if you are not freezing your rice this is your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>id: 9RO83PiuQ-A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525</th>\n",
       "      <td>1711</td>\n",
       "      <td>do you need a super simple thinner and I mean ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>1712</td>\n",
       "      <td>id: 9r3uCzlBHIg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527</th>\n",
       "      <td>1713</td>\n",
       "      <td>I'm a dietitian and a mom who does baby lead w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528</th>\n",
       "      <td>1715</td>\n",
       "      <td>id: EaOrwg8zwSk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1529</th>\n",
       "      <td>1716</td>\n",
       "      <td>man i think i maybe need to lose some weight h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1530 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                                  0\n",
       "0              0                                    id: do8O1YxzdVg\n",
       "1              1  if you don't like eating veggies this video is...\n",
       "2              2                                    id: bpsKYwQffes\n",
       "3              3  if you are not freezing your rice this is your...\n",
       "4              4                                    id: 9RO83PiuQ-A\n",
       "...          ...                                                ...\n",
       "1525        1711  do you need a super simple thinner and I mean ...\n",
       "1526        1712                                    id: 9r3uCzlBHIg\n",
       "1527        1713  I'm a dietitian and a mom who does baby lead w...\n",
       "1528        1715                                    id: EaOrwg8zwSk\n",
       "1529        1716  man i think i maybe need to lose some weight h...\n",
       "\n",
       "[1530 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "info_transcripts = pd.read_csv('all_info_transcripts_new.csv')\n",
    "info_transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vn/7fmw_31101zcswm89qywd8ph0000gn/T/ipykernel_71775/4227489067.py:12: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  id_value = row[0].split(': ')[1] if ': ' in row[0] else None\n",
      "/var/folders/vn/7fmw_31101zcswm89qywd8ph0000gn/T/ipykernel_71775/4227489067.py:23: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  info_transcripts['ID'] = info_transcripts['ID'].fillna(method='ffill')\n",
      "/var/folders/vn/7fmw_31101zcswm89qywd8ph0000gn/T/ipykernel_71775/4227489067.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  infotranscripts_df.rename(columns={'0': 'transcript'}, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do8O1YxzdVg</td>\n",
       "      <td>if you don't like eating veggies this video is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bpsKYwQffes</td>\n",
       "      <td>if you are not freezing your rice this is your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9RO83PiuQ-A</td>\n",
       "      <td>if you don't like cooking and washing dishes b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dI-Be4IehyQ</td>\n",
       "      <td>you've heard of sheep pan dinners but have you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O_7ZvwhzqGg</td>\n",
       "      <td>if you like salads but hate preparing them thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>h1HN1mUtlBE</td>\n",
       "      <td>here's what your favorite foods say about you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>0pCBZYrGWqY</td>\n",
       "      <td>hear my thoughts on nutrition Trends and fads ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>wkvxzcQEXoE</td>\n",
       "      <td>do you need a super simple thinner and I mean ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>9r3uCzlBHIg</td>\n",
       "      <td>I'm a dietitian and a mom who does baby lead w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>EaOrwg8zwSk</td>\n",
       "      <td>man i think i maybe need to lose some weight h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>765 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                         transcript\n",
       "0    do8O1YxzdVg  if you don't like eating veggies this video is...\n",
       "1    bpsKYwQffes  if you are not freezing your rice this is your...\n",
       "2    9RO83PiuQ-A  if you don't like cooking and washing dishes b...\n",
       "3    dI-Be4IehyQ  you've heard of sheep pan dinners but have you...\n",
       "4    O_7ZvwhzqGg  if you like salads but hate preparing them thi...\n",
       "..           ...                                                ...\n",
       "760  h1HN1mUtlBE  here's what your favorite foods say about you ...\n",
       "761  0pCBZYrGWqY  hear my thoughts on nutrition Trends and fads ...\n",
       "762  wkvxzcQEXoE  do you need a super simple thinner and I mean ...\n",
       "763  9r3uCzlBHIg  I'm a dietitian and a mom who does baby lead w...\n",
       "764  EaOrwg8zwSk  man i think i maybe need to lose some weight h...\n",
       "\n",
       "[765 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleaning for informative transcripts\n",
    "# drop Unnamed: 0 column\n",
    "info_transcripts.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "# Create an Id column\n",
    "ids = []\n",
    "# Iterate over the rows of the DataFrame\n",
    "for i, row in info_transcripts.iterrows():\n",
    "    # Check if the row index is even and if the row has at least one element\n",
    "    if i % 2 == 0 and len(row) > 0:\n",
    "        # Split the first element of the row by ': ' and get the second part if it exists\n",
    "        id_value = row[0].split(': ')[1] if ': ' in row[0] else None\n",
    "        # Append the ID to the list\n",
    "        ids.append(id_value)\n",
    "    else:\n",
    "        # Append None if ID is not present in the row or if it's an odd-numbered row\n",
    "        ids.append(None)\n",
    "\n",
    "# Create a new column with the extracted ID values\n",
    "info_transcripts['ID'] = ids\n",
    "\n",
    "# fill odd-numbered rows with ids\n",
    "info_transcripts['ID'] = info_transcripts['ID'].fillna(method='ffill')\n",
    "\n",
    "# Delete even-numbered rows\n",
    "infotranscripts_df = info_transcripts[~(info_transcripts.index % 2 == 0)]\n",
    "\n",
    "#  Reset the index to make it consecutive\n",
    "infotranscripts_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Rename the column named '0' to 'transcripts'\n",
    "infotranscripts_df.rename(columns={'0': 'transcript'}, inplace=True)\n",
    "\n",
    "#reorder columns\n",
    "infotranscripts_df = infotranscripts_df[['ID', 'transcript']]\n",
    "\n",
    "infotranscripts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>id: zqdxQWTdIM4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm going to show you the absolute best natura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>id: aYV9EWaiz_Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I'm going to give you three tips to help you l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>id: G4guVvCYAEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2533</th>\n",
       "      <td>2591</td>\n",
       "      <td>here are five simple steps to help you create ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2534</th>\n",
       "      <td>2592</td>\n",
       "      <td>id: ffF45hF4NV0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2535</th>\n",
       "      <td>2593</td>\n",
       "      <td>chase your dreams not excuses stop making reas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536</th>\n",
       "      <td>2594</td>\n",
       "      <td>id: c956Rmjfquw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537</th>\n",
       "      <td>2595</td>\n",
       "      <td>five habits of successful people one they set ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2538 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                                  0\n",
       "0              0                                    id: zqdxQWTdIM4\n",
       "1              1  I'm going to show you the absolute best natura...\n",
       "2              2                                    id: aYV9EWaiz_Y\n",
       "3              3  I'm going to give you three tips to help you l...\n",
       "4              4                                    id: G4guVvCYAEA\n",
       "...          ...                                                ...\n",
       "2533        2591  here are five simple steps to help you create ...\n",
       "2534        2592                                    id: ffF45hF4NV0\n",
       "2535        2593  chase your dreams not excuses stop making reas...\n",
       "2536        2594                                    id: c956Rmjfquw\n",
       "2537        2595  five habits of successful people one they set ...\n",
       "\n",
       "[2538 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mis_transcripts = pd.read_csv('all_mis_transcripts_new.csv')\n",
    "mis_transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vn/7fmw_31101zcswm89qywd8ph0000gn/T/ipykernel_71775/2117037906.py:12: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  id_value = row[0].split(': ')[1] if ': ' in row[0] else None\n",
      "/var/folders/vn/7fmw_31101zcswm89qywd8ph0000gn/T/ipykernel_71775/2117037906.py:23: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  mis_transcripts['ID'] = mis_transcripts['ID'].fillna(method='ffill')\n",
      "/var/folders/vn/7fmw_31101zcswm89qywd8ph0000gn/T/ipykernel_71775/2117037906.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  misinfotranscripts_df.rename(columns={'0': 'transcript'}, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zqdxQWTdIM4</td>\n",
       "      <td>I'm going to show you the absolute best natura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aYV9EWaiz_Y</td>\n",
       "      <td>I'm going to give you three tips to help you l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>G4guVvCYAEA</td>\n",
       "      <td>the absolute number one most inflammatory food...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u0-U3-f4VHM</td>\n",
       "      <td>you know if you went to your kitchen right now...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llDg68l626M</td>\n",
       "      <td>all right we're at our favorite restaurant KFC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>3SHtxj9nRpM</td>\n",
       "      <td>here are the top 5 foods to stay away from in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>9eeNkQNtfxY</td>\n",
       "      <td>are you looking to improve your diet but don't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>PdLrENnDPbA</td>\n",
       "      <td>here are five simple steps to help you create ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>ffF45hF4NV0</td>\n",
       "      <td>chase your dreams not excuses stop making reas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>c956Rmjfquw</td>\n",
       "      <td>five habits of successful people one they set ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1269 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID                                         transcript\n",
       "0     zqdxQWTdIM4  I'm going to show you the absolute best natura...\n",
       "1     aYV9EWaiz_Y  I'm going to give you three tips to help you l...\n",
       "2     G4guVvCYAEA  the absolute number one most inflammatory food...\n",
       "3     u0-U3-f4VHM  you know if you went to your kitchen right now...\n",
       "4     llDg68l626M  all right we're at our favorite restaurant KFC...\n",
       "...           ...                                                ...\n",
       "1264  3SHtxj9nRpM  here are the top 5 foods to stay away from in ...\n",
       "1265  9eeNkQNtfxY  are you looking to improve your diet but don't...\n",
       "1266  PdLrENnDPbA  here are five simple steps to help you create ...\n",
       "1267  ffF45hF4NV0  chase your dreams not excuses stop making reas...\n",
       "1268  c956Rmjfquw  five habits of successful people one they set ...\n",
       "\n",
       "[1269 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleaning for informative transcripts\n",
    "# drop Unnamed: 0 column\n",
    "mis_transcripts.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "# Create an Id column\n",
    "ids = []\n",
    "# Iterate over the rows of the DataFrame\n",
    "for i, row in mis_transcripts.iterrows():\n",
    "    # Check if the row index is even and if the row has at least one element\n",
    "    if i % 2 == 0 and len(row) > 0:\n",
    "        # Split the first element of the row by ': ' and get the second part if it exists\n",
    "        id_value = row[0].split(': ')[1] if ': ' in row[0] else None\n",
    "        # Append the ID to the list\n",
    "        ids.append(id_value)\n",
    "    else:\n",
    "        # Append None if ID is not present in the row or if it's an odd-numbered row\n",
    "        ids.append(None)\n",
    "\n",
    "# Create a new column with the extracted ID values\n",
    "mis_transcripts['ID'] = ids\n",
    "\n",
    "# fill odd-numbered rows with ids\n",
    "mis_transcripts['ID'] = mis_transcripts['ID'].fillna(method='ffill')\n",
    "\n",
    "# Delete even-numbered rows\n",
    "misinfotranscripts_df = mis_transcripts[~(mis_transcripts.index % 2 == 0)]\n",
    "\n",
    "#  Reset the index to make it consecutive\n",
    "misinfotranscripts_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Rename the column named '0' to 'transcripts'\n",
    "misinfotranscripts_df.rename(columns={'0': 'transcript'}, inplace=True)\n",
    "\n",
    "#reorder columns\n",
    "misinfotranscripts_df = misinfotranscripts_df[['ID', 'transcript']]\n",
    "\n",
    "misinfotranscripts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Punctuation To Transcripts\n",
    "For sentiment analysis on a sentence level, we need to add punctuation back to the transcript for the sentence tokenizer to work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# pip install deepmultilingualpunctuation\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeepmultilingualpunctuation\u001b[39;00m \u001b[39mimport\u001b[39;00m PunctuationModel\n\u001b[0;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m PunctuationModel()\n\u001b[1;32m      5\u001b[0m text \u001b[39m=\u001b[39m misinfotranscripts_df[\u001b[39m'\u001b[39m\u001b[39mtranscript\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m]\n\u001b[1;32m      6\u001b[0m result \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mrestore_punctuation(text)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/deepmultilingualpunctuation/punctuationmodel.py:11\u001b[0m, in \u001b[0;36mPunctuationModel.__init__\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpipe \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39m\u001b[39mner\u001b[39m\u001b[39m\"\u001b[39m,model, grouped_entities\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, device\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpipe \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39m\u001b[39mner\u001b[39m\u001b[39m\"\u001b[39m,model, grouped_entities\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/pipelines/__init__.py:906\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m framework \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    905\u001b[0m     model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[0;32m--> 906\u001b[0m     framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[1;32m    907\u001b[0m         model,\n\u001b[1;32m    908\u001b[0m         model_classes\u001b[39m=\u001b[39mmodel_classes,\n\u001b[1;32m    909\u001b[0m         config\u001b[39m=\u001b[39mconfig,\n\u001b[1;32m    910\u001b[0m         framework\u001b[39m=\u001b[39mframework,\n\u001b[1;32m    911\u001b[0m         task\u001b[39m=\u001b[39mtask,\n\u001b[1;32m    912\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs,\n\u001b[1;32m    913\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m    914\u001b[0m     )\n\u001b[1;32m    916\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n\u001b[1;32m    917\u001b[0m hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:283\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTrying to load the model with Tensorflow.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[1;32m    282\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 283\u001b[0m     model \u001b[39m=\u001b[39m model_class\u001b[39m.\u001b[39mfrom_pretrained(model, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    284\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39meval\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    285\u001b[0m         model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    564\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    565\u001b[0m     )\n\u001b[1;32m    566\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:3677\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3668\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3669\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3670\u001b[0m     (\n\u001b[1;32m   3671\u001b[0m         model,\n\u001b[1;32m   3672\u001b[0m         missing_keys,\n\u001b[1;32m   3673\u001b[0m         unexpected_keys,\n\u001b[1;32m   3674\u001b[0m         mismatched_keys,\n\u001b[1;32m   3675\u001b[0m         offload_index,\n\u001b[1;32m   3676\u001b[0m         error_msgs,\n\u001b[0;32m-> 3677\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_load_pretrained_model(\n\u001b[1;32m   3678\u001b[0m         model,\n\u001b[1;32m   3679\u001b[0m         state_dict,\n\u001b[1;32m   3680\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;00m\n\u001b[1;32m   3681\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3682\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3683\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39mignore_mismatched_sizes,\n\u001b[1;32m   3684\u001b[0m         sharded_metadata\u001b[39m=\u001b[39msharded_metadata,\n\u001b[1;32m   3685\u001b[0m         _fast_init\u001b[39m=\u001b[39m_fast_init,\n\u001b[1;32m   3686\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39mlow_cpu_mem_usage,\n\u001b[1;32m   3687\u001b[0m         device_map\u001b[39m=\u001b[39mdevice_map,\n\u001b[1;32m   3688\u001b[0m         offload_folder\u001b[39m=\u001b[39moffload_folder,\n\u001b[1;32m   3689\u001b[0m         offload_state_dict\u001b[39m=\u001b[39moffload_state_dict,\n\u001b[1;32m   3690\u001b[0m         dtype\u001b[39m=\u001b[39mtorch_dtype,\n\u001b[1;32m   3691\u001b[0m         hf_quantizer\u001b[39m=\u001b[39mhf_quantizer,\n\u001b[1;32m   3692\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39mkeep_in_fp32_modules,\n\u001b[1;32m   3693\u001b[0m     )\n\u001b[1;32m   3695\u001b[0m \u001b[39m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3696\u001b[0m model\u001b[39m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:4050\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   4040\u001b[0m \u001b[39mif\u001b[39;00m state_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   4041\u001b[0m     \u001b[39m# Whole checkpoint\u001b[39;00m\n\u001b[1;32m   4042\u001b[0m     mismatched_keys \u001b[39m=\u001b[39m _find_mismatched_keys(\n\u001b[1;32m   4043\u001b[0m         state_dict,\n\u001b[1;32m   4044\u001b[0m         model_state_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4048\u001b[0m         ignore_mismatched_sizes,\n\u001b[1;32m   4049\u001b[0m     )\n\u001b[0;32m-> 4050\u001b[0m     error_msgs \u001b[39m=\u001b[39m _load_state_dict_into_model(model_to_load, state_dict, start_prefix)\n\u001b[1;32m   4051\u001b[0m     offload_index \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   4052\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4053\u001b[0m     \u001b[39m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n\u001b[1;32m   4054\u001b[0m \n\u001b[1;32m   4055\u001b[0m     \u001b[39m# This should always be a list but, just to be sure.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:702\u001b[0m, in \u001b[0;36m_load_state_dict_into_model\u001b[0;34m(model_to_load, state_dict, start_prefix)\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    700\u001b[0m             load(child, state_dict, prefix \u001b[39m+\u001b[39m name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 702\u001b[0m load(model_to_load, state_dict, prefix\u001b[39m=\u001b[39mstart_prefix)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \u001b[39m# it's safe to delete it.\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[39mdel\u001b[39;00m state_dict\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:700\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    699\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 700\u001b[0m         load(child, state_dict, prefix \u001b[39m+\u001b[39m name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:700\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    699\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 700\u001b[0m         load(child, state_dict, prefix \u001b[39m+\u001b[39m name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:700\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    699\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 700\u001b[0m         load(child, state_dict, prefix \u001b[39m+\u001b[39m name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:696\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[0;34m(module, state_dict, prefix)\u001b[0m\n\u001b[1;32m    694\u001b[0m                     module\u001b[39m.\u001b[39m_load_from_state_dict(\u001b[39m*\u001b[39margs)\n\u001b[1;32m    695\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 696\u001b[0m         module\u001b[39m.\u001b[39m_load_from_state_dict(\u001b[39m*\u001b[39margs)\n\u001b[1;32m    698\u001b[0m \u001b[39mfor\u001b[39;00m name, child \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    699\u001b[0m     \u001b[39mif\u001b[39;00m child \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:2075\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[0;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[1;32m   2073\u001b[0m             \u001b[39msetattr\u001b[39m(\u001b[39mself\u001b[39m, name, input_param)\n\u001b[1;32m   2074\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2075\u001b[0m             param\u001b[39m.\u001b[39mcopy_(input_param)\n\u001b[1;32m   2076\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n\u001b[1;32m   2077\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mswapping\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m use_swap_tensors \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcopying\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# pip install deepmultilingualpunctuation\n",
    "from deepmultilingualpunctuation import PunctuationModel\n",
    "\n",
    "model = PunctuationModel()\n",
    "text = misinfotranscripts_df['transcript'].iloc[0]\n",
    "result = model.restore_punctuation(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/transformers/pipelines/token_classification.py:168: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.NONE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>transcript_with_punctuation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zqdxQWTdIM4</td>\n",
       "      <td>I'm going to show you the absolute best natura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aYV9EWaiz_Y</td>\n",
       "      <td>I'm going to give you three tips to help you l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>G4guVvCYAEA</td>\n",
       "      <td>the absolute number one most inflammatory food...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u0-U3-f4VHM</td>\n",
       "      <td>you know, if you went to your kitchen right no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llDg68l626M</td>\n",
       "      <td>all right, we're at our favorite restaurant, K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>3SHtxj9nRpM</td>\n",
       "      <td>here are the top 5 foods to stay away from in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>9eeNkQNtfxY</td>\n",
       "      <td>are you looking to improve your diet but don't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>PdLrENnDPbA</td>\n",
       "      <td>here are five simple steps to help you create ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>ffF45hF4NV0</td>\n",
       "      <td>chase your dreams, not excuses. stop making re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>c956Rmjfquw</td>\n",
       "      <td>five habits of successful people. one: they se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1269 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID                        transcript_with_punctuation\n",
       "0     zqdxQWTdIM4  I'm going to show you the absolute best natura...\n",
       "1     aYV9EWaiz_Y  I'm going to give you three tips to help you l...\n",
       "2     G4guVvCYAEA  the absolute number one most inflammatory food...\n",
       "3     u0-U3-f4VHM  you know, if you went to your kitchen right no...\n",
       "4     llDg68l626M  all right, we're at our favorite restaurant, K...\n",
       "...           ...                                                ...\n",
       "1264  3SHtxj9nRpM  here are the top 5 foods to stay away from in ...\n",
       "1265  9eeNkQNtfxY  are you looking to improve your diet but don't...\n",
       "1266  PdLrENnDPbA  here are five simple steps to help you create ...\n",
       "1267  ffF45hF4NV0  chase your dreams, not excuses. stop making re...\n",
       "1268  c956Rmjfquw  five habits of successful people. one: they se...\n",
       "\n",
       "[1269 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from deepmultilingualpunctuation import PunctuationModel\n",
    "\n",
    "# Initialize the PunctuationModel\n",
    "model = PunctuationModel()\n",
    "\n",
    "# Create an empty list to store dictionaries of results\n",
    "result_list = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in misinfotranscripts_df.iterrows():\n",
    "    # Access the transcript text in the current row\n",
    "    text = row['transcript']\n",
    "    \n",
    "    # Restore punctuation using the model\n",
    "    result = model.restore_punctuation(text)\n",
    "    \n",
    "    # Append the result to the list\n",
    "    result_list.append({'ID': row['ID'], 'transcript_with_punctuation': result})\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "result_df = pd.DataFrame(result_list)\n",
    "\n",
    "# Print the new DataFrame\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('misinfo_trans_with_punctuation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/transformers/pipelines/token_classification.py:168: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.NONE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>transcript_with_punctuation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do8O1YxzdVg</td>\n",
       "      <td>if you don't like eating veggies, this video i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bpsKYwQffes</td>\n",
       "      <td>if you are not freezing your rice, this is you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9RO83PiuQ-A</td>\n",
       "      <td>if you don't like cooking and washing dishes b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dI-Be4IehyQ</td>\n",
       "      <td>you've heard of sheep pan dinners, but have yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O_7ZvwhzqGg</td>\n",
       "      <td>if you like salads but hate preparing them, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>h1HN1mUtlBE</td>\n",
       "      <td>here's what your favorite foods say about you,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>0pCBZYrGWqY</td>\n",
       "      <td>hear my thoughts on nutrition Trends and fads ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>wkvxzcQEXoE</td>\n",
       "      <td>do you need a super simple, thinner, and I mea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>9r3uCzlBHIg</td>\n",
       "      <td>I'm a dietitian and a mom who does baby lead w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>EaOrwg8zwSk</td>\n",
       "      <td>man, i think i maybe need to lose some weight....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>765 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                        transcript_with_punctuation\n",
       "0    do8O1YxzdVg  if you don't like eating veggies, this video i...\n",
       "1    bpsKYwQffes  if you are not freezing your rice, this is you...\n",
       "2    9RO83PiuQ-A  if you don't like cooking and washing dishes b...\n",
       "3    dI-Be4IehyQ  you've heard of sheep pan dinners, but have yo...\n",
       "4    O_7ZvwhzqGg  if you like salads but hate preparing them, th...\n",
       "..           ...                                                ...\n",
       "760  h1HN1mUtlBE  here's what your favorite foods say about you,...\n",
       "761  0pCBZYrGWqY  hear my thoughts on nutrition Trends and fads ...\n",
       "762  wkvxzcQEXoE  do you need a super simple, thinner, and I mea...\n",
       "763  9r3uCzlBHIg  I'm a dietitian and a mom who does baby lead w...\n",
       "764  EaOrwg8zwSk  man, i think i maybe need to lose some weight....\n",
       "\n",
       "[765 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from deepmultilingualpunctuation import PunctuationModel\n",
    "\n",
    "# Initialize the PunctuationModel\n",
    "model2 = PunctuationModel()\n",
    "\n",
    "# Create an empty list to store dictionaries of results\n",
    "result2_list = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in infotranscripts_df.iterrows():\n",
    "    # Access the transcript text in the current row\n",
    "    text = row['transcript']\n",
    "    \n",
    "    # Restore punctuation using the model\n",
    "    result2 = model2.restore_punctuation(text)\n",
    "    \n",
    "    # Append the result to the list\n",
    "    result2_list.append({'ID': row['ID'], 'transcript_with_punctuation': result2})\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "result2_df = pd.DataFrame(result2_list)\n",
    "\n",
    "# Print the new DataFrame\n",
    "result2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2_df.to_csv('info_trans_with_punctuation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that we have discrepancies within the data. I am cleaning them out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we have punctuation added to the transcript data. We can proceed with sentence tokenization. \n",
    "## Sentence Tokenization\n",
    "### Informative Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>transcript_with_punctuation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do8O1YxzdVg</td>\n",
       "      <td>if you don't like eating veggies, this video i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bpsKYwQffes</td>\n",
       "      <td>if you are not freezing your rice, this is you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9RO83PiuQ-A</td>\n",
       "      <td>if you don't like cooking and washing dishes b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dI-Be4IehyQ</td>\n",
       "      <td>you've heard of sheep pan dinners, but have yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O_7ZvwhzqGg</td>\n",
       "      <td>if you like salads but hate preparing them, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>h1HN1mUtlBE</td>\n",
       "      <td>here's what your favorite foods say about you,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>0pCBZYrGWqY</td>\n",
       "      <td>hear my thoughts on nutrition Trends and fads ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>wkvxzcQEXoE</td>\n",
       "      <td>do you need a super simple, thinner, and I mea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>9r3uCzlBHIg</td>\n",
       "      <td>I'm a dietitian and a mom who does baby lead w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>EaOrwg8zwSk</td>\n",
       "      <td>man, i think i maybe need to lose some weight....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>726 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                        transcript_with_punctuation\n",
       "0    do8O1YxzdVg  if you don't like eating veggies, this video i...\n",
       "1    bpsKYwQffes  if you are not freezing your rice, this is you...\n",
       "2    9RO83PiuQ-A  if you don't like cooking and washing dishes b...\n",
       "3    dI-Be4IehyQ  you've heard of sheep pan dinners, but have yo...\n",
       "4    O_7ZvwhzqGg  if you like salads but hate preparing them, th...\n",
       "..           ...                                                ...\n",
       "721  h1HN1mUtlBE  here's what your favorite foods say about you,...\n",
       "722  0pCBZYrGWqY  hear my thoughts on nutrition Trends and fads ...\n",
       "723  wkvxzcQEXoE  do you need a super simple, thinner, and I mea...\n",
       "724  9r3uCzlBHIg  I'm a dietitian and a mom who does baby lead w...\n",
       "725  EaOrwg8zwSk  man, i think i maybe need to lose some weight....\n",
       "\n",
       "[726 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "infotranscripts_df = pd.read_csv('info_trans_with_punctuation_cleaned.csv')\n",
    "infotranscripts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jiyoonkim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jiyoonkim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jiyoonkim/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>transcript_with_punctuation</th>\n",
       "      <th>original_sentence</th>\n",
       "      <th>tokenized_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do8O1YxzdVg</td>\n",
       "      <td>if you don't like eating veggies, this video i...</td>\n",
       "      <td>if you don't like eating veggies, this video i...</td>\n",
       "      <td>if you dont like eating veggie this video is f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do8O1YxzdVg</td>\n",
       "      <td>if you don't like eating veggies, this video i...</td>\n",
       "      <td>as a dietitian, one of the most common things ...</td>\n",
       "      <td>a a dietitian one of the most common thing i h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do8O1YxzdVg</td>\n",
       "      <td>if you don't like eating veggies, this video i...</td>\n",
       "      <td>for example, make a chopped salad box and use ...</td>\n",
       "      <td>for example make a chopped salad box and use t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do8O1YxzdVg</td>\n",
       "      <td>if you don't like eating veggies, this video i...</td>\n",
       "      <td>I'm using cabbage, cucumber, broccoli, avocado.</td>\n",
       "      <td>im using cabbage cucumber broccoli avocado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do8O1YxzdVg</td>\n",
       "      <td>if you don't like eating veggies, this video i...</td>\n",
       "      <td>I'm just going to top this with chickpeas and ...</td>\n",
       "      <td>im just going to top this with chickpea and pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>EaOrwg8zwSk</td>\n",
       "      <td>man, i think i maybe need to lose some weight....</td>\n",
       "      <td>i won't make you restrict anything and i won't...</td>\n",
       "      <td>i wont make you restrict anything and i wont m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>EaOrwg8zwSk</td>\n",
       "      <td>man, i think i maybe need to lose some weight....</td>\n",
       "      <td>get a load of this guy.</td>\n",
       "      <td>get a load of this guy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>EaOrwg8zwSk</td>\n",
       "      <td>man, i think i maybe need to lose some weight....</td>\n",
       "      <td>she steals everyone from us.</td>\n",
       "      <td>she steal everyone from u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>EaOrwg8zwSk</td>\n",
       "      <td>man, i think i maybe need to lose some weight....</td>\n",
       "      <td>i like you.</td>\n",
       "      <td>i like you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>EaOrwg8zwSk</td>\n",
       "      <td>man, i think i maybe need to lose some weight....</td>\n",
       "      <td>i think you're gonna be sustainable for me, re...</td>\n",
       "      <td>i think youre gon na be sustainable for me rea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8083 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                        transcript_with_punctuation  \\\n",
       "0    do8O1YxzdVg  if you don't like eating veggies, this video i...   \n",
       "0    do8O1YxzdVg  if you don't like eating veggies, this video i...   \n",
       "0    do8O1YxzdVg  if you don't like eating veggies, this video i...   \n",
       "0    do8O1YxzdVg  if you don't like eating veggies, this video i...   \n",
       "0    do8O1YxzdVg  if you don't like eating veggies, this video i...   \n",
       "..           ...                                                ...   \n",
       "725  EaOrwg8zwSk  man, i think i maybe need to lose some weight....   \n",
       "725  EaOrwg8zwSk  man, i think i maybe need to lose some weight....   \n",
       "725  EaOrwg8zwSk  man, i think i maybe need to lose some weight....   \n",
       "725  EaOrwg8zwSk  man, i think i maybe need to lose some weight....   \n",
       "725  EaOrwg8zwSk  man, i think i maybe need to lose some weight....   \n",
       "\n",
       "                                     original_sentence  \\\n",
       "0    if you don't like eating veggies, this video i...   \n",
       "0    as a dietitian, one of the most common things ...   \n",
       "0    for example, make a chopped salad box and use ...   \n",
       "0      I'm using cabbage, cucumber, broccoli, avocado.   \n",
       "0    I'm just going to top this with chickpeas and ...   \n",
       "..                                                 ...   \n",
       "725  i won't make you restrict anything and i won't...   \n",
       "725                            get a load of this guy.   \n",
       "725                       she steals everyone from us.   \n",
       "725                                        i like you.   \n",
       "725  i think you're gonna be sustainable for me, re...   \n",
       "\n",
       "                                    tokenized_sentence  \n",
       "0    if you dont like eating veggie this video is f...  \n",
       "0    a a dietitian one of the most common thing i h...  \n",
       "0    for example make a chopped salad box and use t...  \n",
       "0           im using cabbage cucumber broccoli avocado  \n",
       "0    im just going to top this with chickpea and pu...  \n",
       "..                                                 ...  \n",
       "725  i wont make you restrict anything and i wont m...  \n",
       "725                             get a load of this guy  \n",
       "725                          she steal everyone from u  \n",
       "725                                         i like you  \n",
       "725  i think youre gon na be sustainable for me rea...  \n",
       "\n",
       "[8083 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "# Download NLTK resources if not already downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess text at the sentence level\n",
    "# Function to preprocess text at the sentence level\n",
    "def preprocess_text_column(df, column_name):\n",
    "  \"\"\"\n",
    "  This function takes a dataframe (df) and a column name (column_name) containing text data as input.\n",
    "  It performs sentence tokenization, explodes the dataframe by sentence, and then preprocesses each sentence.\n",
    "  \"\"\"\n",
    "  df['original_sentence'] = df[column_name].apply(sent_tokenize)\n",
    "  # Explode the dataframe by sentence\n",
    "  exploded_df = df.explode('original_sentence')\n",
    "  \n",
    "  def preprocess_sentence(sentence):\n",
    "    # Text preprocessing steps (same as before)\n",
    "    sentence = emoji.demojize(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    clean_sentence = re.sub(r'[^a-zA-Z0-9\\s]', '', sentence)\n",
    "    tokens = word_tokenize(clean_sentence)\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "    # filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Keep all tokens\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "  # Apply preprocessing to each sentence in the exploded dataframe\n",
    "  exploded_df['tokenized_sentence'] = exploded_df['original_sentence'].apply(preprocess_sentence)\n",
    "  \n",
    "  # Drop the original sentence column if not needed (optional)\n",
    "  # exploded_df.drop('original_sentence', axis=1, inplace=True)  # Uncomment to drop the original sentence column\n",
    "\n",
    "  return exploded_df\n",
    "\n",
    "infotranscripts_df = preprocess_text_column(infotranscripts_df.copy(), 'transcript_with_punctuation')\n",
    "infotranscripts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>original_sentence</th>\n",
       "      <th>tokenized_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do8O1YxzdVg</td>\n",
       "      <td>if you don't like eating veggies, this video i...</td>\n",
       "      <td>if you dont like eating veggie this video is f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do8O1YxzdVg</td>\n",
       "      <td>as a dietitian, one of the most common things ...</td>\n",
       "      <td>a a dietitian one of the most common thing i h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do8O1YxzdVg</td>\n",
       "      <td>for example, make a chopped salad box and use ...</td>\n",
       "      <td>for example make a chopped salad box and use t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do8O1YxzdVg</td>\n",
       "      <td>I'm using cabbage, cucumber, broccoli, avocado.</td>\n",
       "      <td>im using cabbage cucumber broccoli avocado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do8O1YxzdVg</td>\n",
       "      <td>I'm just going to top this with chickpeas and ...</td>\n",
       "      <td>im just going to top this with chickpea and pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>EaOrwg8zwSk</td>\n",
       "      <td>i won't make you restrict anything and i won't...</td>\n",
       "      <td>i wont make you restrict anything and i wont m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>EaOrwg8zwSk</td>\n",
       "      <td>get a load of this guy.</td>\n",
       "      <td>get a load of this guy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>EaOrwg8zwSk</td>\n",
       "      <td>she steals everyone from us.</td>\n",
       "      <td>she steal everyone from u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>EaOrwg8zwSk</td>\n",
       "      <td>i like you.</td>\n",
       "      <td>i like you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>EaOrwg8zwSk</td>\n",
       "      <td>i think you're gonna be sustainable for me, re...</td>\n",
       "      <td>i think youre gon na be sustainable for me rea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8083 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                  original_sentence  \\\n",
       "0    do8O1YxzdVg  if you don't like eating veggies, this video i...   \n",
       "0    do8O1YxzdVg  as a dietitian, one of the most common things ...   \n",
       "0    do8O1YxzdVg  for example, make a chopped salad box and use ...   \n",
       "0    do8O1YxzdVg    I'm using cabbage, cucumber, broccoli, avocado.   \n",
       "0    do8O1YxzdVg  I'm just going to top this with chickpeas and ...   \n",
       "..           ...                                                ...   \n",
       "725  EaOrwg8zwSk  i won't make you restrict anything and i won't...   \n",
       "725  EaOrwg8zwSk                            get a load of this guy.   \n",
       "725  EaOrwg8zwSk                       she steals everyone from us.   \n",
       "725  EaOrwg8zwSk                                        i like you.   \n",
       "725  EaOrwg8zwSk  i think you're gonna be sustainable for me, re...   \n",
       "\n",
       "                                    tokenized_sentence  \n",
       "0    if you dont like eating veggie this video is f...  \n",
       "0    a a dietitian one of the most common thing i h...  \n",
       "0    for example make a chopped salad box and use t...  \n",
       "0           im using cabbage cucumber broccoli avocado  \n",
       "0    im just going to top this with chickpea and pu...  \n",
       "..                                                 ...  \n",
       "725  i wont make you restrict anything and i wont m...  \n",
       "725                             get a load of this guy  \n",
       "725                          she steal everyone from u  \n",
       "725                                         i like you  \n",
       "725  i think youre gon na be sustainable for me rea...  \n",
       "\n",
       "[8083 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infotranscripts_df = infotranscripts_df.drop(['transcript_with_punctuation'], axis=1)\n",
    "infotranscripts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of tokenized sentences. We calculate the sentiment score of each sentence using Textblob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>original_sentence</th>\n",
       "      <th>tokenized_sentence</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do8O1YxzdVg</td>\n",
       "      <td>if you don't like eating veggies, this video i...</td>\n",
       "      <td>if you dont like eating veggie this video is f...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do8O1YxzdVg</td>\n",
       "      <td>as a dietitian, one of the most common things ...</td>\n",
       "      <td>a a dietitian one of the most common thing i h...</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do8O1YxzdVg</td>\n",
       "      <td>for example, make a chopped salad box and use ...</td>\n",
       "      <td>for example make a chopped salad box and use t...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do8O1YxzdVg</td>\n",
       "      <td>I'm using cabbage, cucumber, broccoli, avocado.</td>\n",
       "      <td>im using cabbage cucumber broccoli avocado</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>do8O1YxzdVg</td>\n",
       "      <td>I'm just going to top this with chickpeas and ...</td>\n",
       "      <td>im just going to top this with chickpea and pu...</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>EaOrwg8zwSk</td>\n",
       "      <td>i won't make you restrict anything and i won't...</td>\n",
       "      <td>i wont make you restrict anything and i wont m...</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>EaOrwg8zwSk</td>\n",
       "      <td>get a load of this guy.</td>\n",
       "      <td>get a load of this guy</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>EaOrwg8zwSk</td>\n",
       "      <td>she steals everyone from us.</td>\n",
       "      <td>she steal everyone from u</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>EaOrwg8zwSk</td>\n",
       "      <td>i like you.</td>\n",
       "      <td>i like you</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>EaOrwg8zwSk</td>\n",
       "      <td>i think you're gonna be sustainable for me, re...</td>\n",
       "      <td>i think youre gon na be sustainable for me rea...</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8083 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                  original_sentence  \\\n",
       "0    do8O1YxzdVg  if you don't like eating veggies, this video i...   \n",
       "0    do8O1YxzdVg  as a dietitian, one of the most common things ...   \n",
       "0    do8O1YxzdVg  for example, make a chopped salad box and use ...   \n",
       "0    do8O1YxzdVg    I'm using cabbage, cucumber, broccoli, avocado.   \n",
       "0    do8O1YxzdVg  I'm just going to top this with chickpeas and ...   \n",
       "..           ...                                                ...   \n",
       "725  EaOrwg8zwSk  i won't make you restrict anything and i won't...   \n",
       "725  EaOrwg8zwSk                            get a load of this guy.   \n",
       "725  EaOrwg8zwSk                       she steals everyone from us.   \n",
       "725  EaOrwg8zwSk                                        i like you.   \n",
       "725  EaOrwg8zwSk  i think you're gonna be sustainable for me, re...   \n",
       "\n",
       "                                    tokenized_sentence  sentiment_score  \n",
       "0    if you dont like eating veggie this video is f...             0.00  \n",
       "0    a a dietitian one of the most common thing i h...             0.05  \n",
       "0    for example make a chopped salad box and use t...             0.00  \n",
       "0           im using cabbage cucumber broccoli avocado             0.00  \n",
       "0    im just going to top this with chickpea and pu...             0.50  \n",
       "..                                                 ...              ...  \n",
       "725  i wont make you restrict anything and i wont m...            -1.00  \n",
       "725                             get a load of this guy             0.00  \n",
       "725                          she steal everyone from u             0.00  \n",
       "725                                         i like you             0.00  \n",
       "725  i think youre gon na be sustainable for me rea...             0.20  \n",
       "\n",
       "[8083 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob #!pip install textblob\n",
    "\n",
    "def calculate_sentiment_score(text):\n",
    "  # Text is now a single sentence (not a list)\n",
    "  blob = TextBlob(text)\n",
    "  return blob.sentiment.polarity\n",
    "\n",
    "\n",
    "infotranscripts_df['sentiment_score'] = infotranscripts_df['tokenized_sentence'].apply(calculate_sentiment_score)\n",
    "infotranscripts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infotranscripts_df = infotranscripts_df.drop(['transcript_with_punctuation'], axis=1)\n",
    "# infotranscripts_df.to_csv('sentence-token-info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have sentiment scores calculated for each sentence in a video. \n",
    "<br> We repeat the process with misinformative transcripts.\n",
    "### Misinformative Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>original_sentence</th>\n",
       "      <th>tokenized_sentence</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zqdxQWTdIM4</td>\n",
       "      <td>I'm going to show you the absolute best natura...</td>\n",
       "      <td>im going to show you the absolute best natural...</td>\n",
       "      <td>0.244615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zqdxQWTdIM4</td>\n",
       "      <td>normally it's stiff from all the injuries.</td>\n",
       "      <td>normally it stiff from all the injury</td>\n",
       "      <td>-0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zqdxQWTdIM4</td>\n",
       "      <td>so this nutrient is a precursor to Something i...</td>\n",
       "      <td>so this nutrient is a precursor to something i...</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zqdxQWTdIM4</td>\n",
       "      <td>if you're deficient in NAD, you get low carage...</td>\n",
       "      <td>if youre deficient in nad you get low carage r...</td>\n",
       "      <td>-0.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zqdxQWTdIM4</td>\n",
       "      <td>the precursor for NAD that you need to take is...</td>\n",
       "      <td>the precursor for nad that you need to take is...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>c956Rmjfquw</td>\n",
       "      <td>five: they are Learners.</td>\n",
       "      <td>five they are learner</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>c956Rmjfquw</td>\n",
       "      <td>they never stop learning.</td>\n",
       "      <td>they never stop learning</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>c956Rmjfquw</td>\n",
       "      <td>they are curious, open-minded and always looki...</td>\n",
       "      <td>they are curious openminded and always looking...</td>\n",
       "      <td>-0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>c956Rmjfquw</td>\n",
       "      <td>in the next video, we will teach you how to de...</td>\n",
       "      <td>in the next video we will teach you how to dev...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>c956Rmjfquw</td>\n",
       "      <td>thanks for.</td>\n",
       "      <td>thanks for</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12604 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID                                  original_sentence  \\\n",
       "0     zqdxQWTdIM4  I'm going to show you the absolute best natura...   \n",
       "0     zqdxQWTdIM4         normally it's stiff from all the injuries.   \n",
       "0     zqdxQWTdIM4  so this nutrient is a precursor to Something i...   \n",
       "0     zqdxQWTdIM4  if you're deficient in NAD, you get low carage...   \n",
       "0     zqdxQWTdIM4  the precursor for NAD that you need to take is...   \n",
       "...           ...                                                ...   \n",
       "1256  c956Rmjfquw                           five: they are Learners.   \n",
       "1256  c956Rmjfquw                          they never stop learning.   \n",
       "1256  c956Rmjfquw  they are curious, open-minded and always looki...   \n",
       "1256  c956Rmjfquw  in the next video, we will teach you how to de...   \n",
       "1256  c956Rmjfquw                                        thanks for.   \n",
       "\n",
       "                                     tokenized_sentence  sentiment_score  \n",
       "0     im going to show you the absolute best natural...         0.244615  \n",
       "0                 normally it stiff from all the injury        -0.214286  \n",
       "0     so this nutrient is a precursor to something i...         0.600000  \n",
       "0     if youre deficient in nad you get low carage r...        -0.075000  \n",
       "0     the precursor for nad that you need to take is...         0.000000  \n",
       "...                                                 ...              ...  \n",
       "1256                              five they are learner         0.000000  \n",
       "1256                           they never stop learning         0.000000  \n",
       "1256  they are curious openminded and always looking...        -0.100000  \n",
       "1256  in the next video we will teach you how to dev...         0.000000  \n",
       "1256                                         thanks for         0.200000  \n",
       "\n",
       "[12604 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "mistranscripts_df = pd.read_csv('misinfo_trans_with_punctuation_cleaned.csv')\n",
    "\n",
    "mistranscripts_df = preprocess_text_column(mistranscripts_df.copy(), 'transcript_with_punctuation')\n",
    "mistranscripts_df = mistranscripts_df.drop(['transcript_with_punctuation'], axis=1)\n",
    "mistranscripts_df['sentiment_score'] = mistranscripts_df['tokenized_sentence'].apply(calculate_sentiment_score)\n",
    "mistranscripts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we decide to visualize the distribution of data, using a violin plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>original_sentence</th>\n",
       "      <th>tokenized_sentence</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>is_informative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zqdxQWTdIM4</td>\n",
       "      <td>I'm going to show you the absolute best natura...</td>\n",
       "      <td>im going to show you the absolute best natural...</td>\n",
       "      <td>0.244615</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zqdxQWTdIM4</td>\n",
       "      <td>normally it's stiff from all the injuries.</td>\n",
       "      <td>normally it stiff from all the injury</td>\n",
       "      <td>-0.214286</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zqdxQWTdIM4</td>\n",
       "      <td>so this nutrient is a precursor to Something i...</td>\n",
       "      <td>so this nutrient is a precursor to something i...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zqdxQWTdIM4</td>\n",
       "      <td>if you're deficient in NAD, you get low carage...</td>\n",
       "      <td>if youre deficient in nad you get low carage r...</td>\n",
       "      <td>-0.075000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zqdxQWTdIM4</td>\n",
       "      <td>the precursor for NAD that you need to take is...</td>\n",
       "      <td>the precursor for nad that you need to take is...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20682</th>\n",
       "      <td>EaOrwg8zwSk</td>\n",
       "      <td>i won't make you restrict anything and i won't...</td>\n",
       "      <td>i wont make you restrict anything and i wont m...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20683</th>\n",
       "      <td>EaOrwg8zwSk</td>\n",
       "      <td>get a load of this guy.</td>\n",
       "      <td>get a load of this guy</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20684</th>\n",
       "      <td>EaOrwg8zwSk</td>\n",
       "      <td>she steals everyone from us.</td>\n",
       "      <td>she steal everyone from u</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20685</th>\n",
       "      <td>EaOrwg8zwSk</td>\n",
       "      <td>i like you.</td>\n",
       "      <td>i like you</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20686</th>\n",
       "      <td>EaOrwg8zwSk</td>\n",
       "      <td>i think you're gonna be sustainable for me, re...</td>\n",
       "      <td>i think youre gon na be sustainable for me rea...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20687 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID                                  original_sentence  \\\n",
       "0      zqdxQWTdIM4  I'm going to show you the absolute best natura...   \n",
       "1      zqdxQWTdIM4         normally it's stiff from all the injuries.   \n",
       "2      zqdxQWTdIM4  so this nutrient is a precursor to Something i...   \n",
       "3      zqdxQWTdIM4  if you're deficient in NAD, you get low carage...   \n",
       "4      zqdxQWTdIM4  the precursor for NAD that you need to take is...   \n",
       "...            ...                                                ...   \n",
       "20682  EaOrwg8zwSk  i won't make you restrict anything and i won't...   \n",
       "20683  EaOrwg8zwSk                            get a load of this guy.   \n",
       "20684  EaOrwg8zwSk                       she steals everyone from us.   \n",
       "20685  EaOrwg8zwSk                                        i like you.   \n",
       "20686  EaOrwg8zwSk  i think you're gonna be sustainable for me, re...   \n",
       "\n",
       "                                      tokenized_sentence  sentiment_score  \\\n",
       "0      im going to show you the absolute best natural...         0.244615   \n",
       "1                  normally it stiff from all the injury        -0.214286   \n",
       "2      so this nutrient is a precursor to something i...         0.600000   \n",
       "3      if youre deficient in nad you get low carage r...        -0.075000   \n",
       "4      the precursor for nad that you need to take is...         0.000000   \n",
       "...                                                  ...              ...   \n",
       "20682  i wont make you restrict anything and i wont m...        -1.000000   \n",
       "20683                             get a load of this guy         0.000000   \n",
       "20684                          she steal everyone from u         0.000000   \n",
       "20685                                         i like you         0.000000   \n",
       "20686  i think youre gon na be sustainable for me rea...         0.200000   \n",
       "\n",
       "       is_informative  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   0  \n",
       "4                   0  \n",
       "...               ...  \n",
       "20682               1  \n",
       "20683               1  \n",
       "20684               1  \n",
       "20685               1  \n",
       "20686               1  \n",
       "\n",
       "[20687 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combined dataframe for testing\n",
    "mistranscripts_df['is_informative'] = int(False)\n",
    "infotranscripts_df['is_informative'] = int(True)\n",
    "\n",
    "combined_df = pd.concat([mistranscripts_df, infotranscripts_df], ignore_index=True)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vn/7fmw_31101zcswm89qywd8ph0000gn/T/ipykernel_71775/1517890511.py:5: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.violinplot(x = \"is_informative\", y = \"sentiment_score\", showmeans=True, data=combined_df, palette=palette)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "PolyCollection.set() got an unexpected keyword argument 'showmeans'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m palette \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mred\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mblue\u001b[39m\u001b[39m'\u001b[39m] \n\u001b[0;32m----> 5\u001b[0m sns\u001b[39m.\u001b[39mviolinplot(x \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mis_informative\u001b[39m\u001b[39m\"\u001b[39m, y \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msentiment_score\u001b[39m\u001b[39m\"\u001b[39m, showmeans\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data\u001b[39m=\u001b[39mcombined_df, palette\u001b[39m=\u001b[39mpalette)\n\u001b[1;32m      6\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39mCategory\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m'\u001b[39m\u001b[39mSentiment Score\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/seaborn/categorical.py:1770\u001b[0m, in \u001b[0;36mviolinplot\u001b[0;34m(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, inner, split, width, dodge, gap, linewidth, linecolor, cut, gridsize, bw_method, bw_adjust, density_norm, common_norm, hue_norm, formatter, log_scale, native_scale, legend, scale, scale_hue, bw, inner_kws, ax, **kwargs)\u001b[0m\n\u001b[1;32m   1767\u001b[0m kde_kws \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(cut\u001b[39m=\u001b[39mcut, gridsize\u001b[39m=\u001b[39mgridsize, bw_method\u001b[39m=\u001b[39mbw_method, bw_adjust\u001b[39m=\u001b[39mbw_adjust)\n\u001b[1;32m   1768\u001b[0m inner_kws \u001b[39m=\u001b[39m {} \u001b[39mif\u001b[39;00m inner_kws \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m inner_kws\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m-> 1770\u001b[0m p\u001b[39m.\u001b[39mplot_violins(\n\u001b[1;32m   1771\u001b[0m     width\u001b[39m=\u001b[39mwidth,\n\u001b[1;32m   1772\u001b[0m     dodge\u001b[39m=\u001b[39mdodge,\n\u001b[1;32m   1773\u001b[0m     gap\u001b[39m=\u001b[39mgap,\n\u001b[1;32m   1774\u001b[0m     split\u001b[39m=\u001b[39msplit,\n\u001b[1;32m   1775\u001b[0m     color\u001b[39m=\u001b[39mcolor,\n\u001b[1;32m   1776\u001b[0m     fill\u001b[39m=\u001b[39mfill,\n\u001b[1;32m   1777\u001b[0m     linecolor\u001b[39m=\u001b[39mlinecolor,\n\u001b[1;32m   1778\u001b[0m     linewidth\u001b[39m=\u001b[39mlinewidth,\n\u001b[1;32m   1779\u001b[0m     inner\u001b[39m=\u001b[39minner,\n\u001b[1;32m   1780\u001b[0m     density_norm\u001b[39m=\u001b[39mdensity_norm,\n\u001b[1;32m   1781\u001b[0m     common_norm\u001b[39m=\u001b[39mcommon_norm,\n\u001b[1;32m   1782\u001b[0m     kde_kws\u001b[39m=\u001b[39mkde_kws,\n\u001b[1;32m   1783\u001b[0m     inner_kws\u001b[39m=\u001b[39minner_kws,\n\u001b[1;32m   1784\u001b[0m     plot_kws\u001b[39m=\u001b[39mkwargs,\n\u001b[1;32m   1785\u001b[0m )\n\u001b[1;32m   1787\u001b[0m p\u001b[39m.\u001b[39m_add_axis_labels(ax)\n\u001b[1;32m   1788\u001b[0m p\u001b[39m.\u001b[39m_adjust_cat_axis(ax, axis\u001b[39m=\u001b[39mp\u001b[39m.\u001b[39morient)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/seaborn/categorical.py:1047\u001b[0m, in \u001b[0;36m_CategoricalPlotter.plot_violins\u001b[0;34m(self, width, dodge, gap, split, color, fill, linecolor, linewidth, inner, density_norm, common_norm, kde_kws, inner_kws, plot_kws)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[39m# Plot the main violin body\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m plot_func \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m: ax\u001b[39m.\u001b[39mfill_betweenx, \u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m: ax\u001b[39m.\u001b[39mfill_between}[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39morient]\n\u001b[0;32m-> 1047\u001b[0m plot_func(\n\u001b[1;32m   1048\u001b[0m     inv_val(data[value_var]),\n\u001b[1;32m   1049\u001b[0m     inv_pos(data[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39morient] \u001b[39m-\u001b[39m offsets[\u001b[39m0\u001b[39m]),\n\u001b[1;32m   1050\u001b[0m     inv_pos(data[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39morient] \u001b[39m+\u001b[39m offsets[\u001b[39m1\u001b[39m]),\n\u001b[1;32m   1051\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mviolin[\u001b[39m\"\u001b[39m\u001b[39mkwargs\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1052\u001b[0m )\n\u001b[1;32m   1054\u001b[0m \u001b[39m# Adjust the observation data\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m obs \u001b[39m=\u001b[39m violin[\u001b[39m\"\u001b[39m\u001b[39mobservations\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/matplotlib/__init__.py:1465\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m   1463\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1464\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1465\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39m\u001b[39mmap\u001b[39m(sanitize_sequence, args), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1467\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1468\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n\u001b[1;32m   1469\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/matplotlib/axes/_axes.py:5518\u001b[0m, in \u001b[0;36mAxes.fill_betweenx\u001b[0;34m(self, y, x1, x2, where, step, interpolate, **kwargs)\u001b[0m\n\u001b[1;32m   5516\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfill_betweenx\u001b[39m(\u001b[39mself\u001b[39m, y, x1, x2\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, where\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   5517\u001b[0m                   step\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, interpolate\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 5518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fill_between_x_or_y(\n\u001b[1;32m   5519\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, y, x1, x2,\n\u001b[1;32m   5520\u001b[0m         where\u001b[39m=\u001b[39mwhere, interpolate\u001b[39m=\u001b[39minterpolate, step\u001b[39m=\u001b[39mstep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/matplotlib/axes/_axes.py:5485\u001b[0m, in \u001b[0;36mAxes._fill_between_x_or_y\u001b[0;34m(self, ind_dir, ind, dep1, dep2, where, interpolate, step, **kwargs)\u001b[0m\n\u001b[1;32m   5481\u001b[0m         pts \u001b[39m=\u001b[39m pts[:, ::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m   5483\u001b[0m     polys\u001b[39m.\u001b[39mappend(pts)\n\u001b[0;32m-> 5485\u001b[0m collection \u001b[39m=\u001b[39m mcoll\u001b[39m.\u001b[39mPolyCollection(polys, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   5487\u001b[0m \u001b[39m# now update the datalim and autoscale\u001b[39;00m\n\u001b[1;32m   5488\u001b[0m pts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack([np\u001b[39m.\u001b[39mhstack([ind[where, \u001b[39mNone\u001b[39;00m], dep1[where, \u001b[39mNone\u001b[39;00m]]),\n\u001b[1;32m   5489\u001b[0m                  np\u001b[39m.\u001b[39mhstack([ind[where, \u001b[39mNone\u001b[39;00m], dep2[where, \u001b[39mNone\u001b[39;00m]])])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/matplotlib/collections.py:1198\u001b[0m, in \u001b[0;36mPolyCollection.__init__\u001b[0;34m(self, verts, sizes, closed, **kwargs)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, verts, sizes\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m, closed\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1179\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m \u001b[39m    ----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[39m        Forwarded to `.Collection`.\u001b[39;00m\n\u001b[1;32m   1197\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1198\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_sizes(sizes)\n\u001b[1;32m   1200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_verts(verts, closed)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/matplotlib/collections.py:203\u001b[0m, in \u001b[0;36mCollection.__init__\u001b[0;34m(self, edgecolors, facecolors, linewidths, linestyles, capstyle, joinstyle, antialiaseds, offsets, offset_transform, norm, cmap, pickradius, hatch, urls, zorder, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_offset_transform \u001b[39m=\u001b[39m offset_transform\n\u001b[1;32m    202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path_effects \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_update(kwargs)\n\u001b[1;32m    204\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_paths \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/matplotlib/artist.py:1219\u001b[0m, in \u001b[0;36mArtist._internal_update\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_internal_update\u001b[39m(\u001b[39mself\u001b[39m, kwargs):\n\u001b[1;32m   1213\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[39m    Update artist properties without prenormalizing them, but generating\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[39m    errors as if calling `set`.\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \n\u001b[1;32m   1217\u001b[0m \u001b[39m    The lack of prenormalization is to maintain backcompatibility.\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1219\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_props(\n\u001b[1;32m   1220\u001b[0m         kwargs, \u001b[39m\"\u001b[39m\u001b[39m{cls.__name__}\u001b[39;00m\u001b[39m.set() got an unexpected keyword argument \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1221\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m{prop_name!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/matplotlib/artist.py:1193\u001b[0m, in \u001b[0;36mArtist._update_props\u001b[0;34m(self, props, errfmt)\u001b[0m\n\u001b[1;32m   1191\u001b[0m             func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mset_\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   1192\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mcallable\u001b[39m(func):\n\u001b[0;32m-> 1193\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m   1194\u001b[0m                     errfmt\u001b[39m.\u001b[39mformat(\u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m), prop_name\u001b[39m=\u001b[39mk))\n\u001b[1;32m   1195\u001b[0m             ret\u001b[39m.\u001b[39mappend(func(v))\n\u001b[1;32m   1196\u001b[0m \u001b[39mif\u001b[39;00m ret:\n",
      "\u001b[0;31mAttributeError\u001b[0m: PolyCollection.set() got an unexpected keyword argument 'showmeans'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "palette = ['red', 'blue'] \n",
    "sns.violinplot(x = \"is_informative\", y = \"sentiment_score\", showmeans=True, data=combined_df, palette=palette)\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Sentiment Score')\n",
    "plt.title('Sentiment Score Distribution by Category (Violin Plot)')\n",
    "plt.xticks(ticks=[0, 1], labels = ['Misinformative', 'Informative'])\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also perform a significance test. The Mann-Whitney U test is used, due to abnormal distribution of the violin plot.\n",
    "A low p-value (typically less than 0.05) suggests a statistically significant difference in sentiment scores between the two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U statistic: 51830878.0000\n",
      "p-value: 0.0266\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "informative_scores = combined_df[combined_df['is_informative'] == 1]['sentiment_score']\n",
    "misinformative_scores = combined_df[combined_df['is_informative'] == 0]['sentiment_score']\n",
    "\n",
    "u_statistic, p_value = stats.mannwhitneyu(informative_scores, misinformative_scores)\n",
    "\n",
    "# Print the results\n",
    "print(f\"U statistic: {u_statistic:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P-value is 0.0266. It is statistically significant. We can continue with the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>original_sentence</th>\n",
       "      <th>tokenized_sentence</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>is_informative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9RO83PiuQ-A</td>\n",
       "      <td>if you don't like cooking and washing dishes b...</td>\n",
       "      <td>if you dont like cooking and washing dish but ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>05J3vTYNqCw</td>\n",
       "      <td>you have this yummy, delicious, chocolatey ras...</td>\n",
       "      <td>you have this yummy delicious chocolatey raspb...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>lZRU50mb0Yg</td>\n",
       "      <td>also had leftover veggies and this delicious s...</td>\n",
       "      <td>also had leftover veggie and this delicious se...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>lZRU50mb0Yg</td>\n",
       "      <td>it is delicious and filling.</td>\n",
       "      <td>it is delicious and filling</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>uWBfiPSmU3s</td>\n",
       "      <td>our bodies need carbs to function optimally an...</td>\n",
       "      <td>our body need carbs to function optimally and ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>0LCXoyHJJQ4</td>\n",
       "      <td>we practice eating the amount that makes our b...</td>\n",
       "      <td>we practice eating the amount that make our bo...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>Pjq7JEHFS5A</td>\n",
       "      <td>yeah, but you'd like never know so good.</td>\n",
       "      <td>yeah but youd like never know so good</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>VIHn1gCdQNk</td>\n",
       "      <td>oh so, so good.</td>\n",
       "      <td>oh so so good</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>-6nB_9dGRZc</td>\n",
       "      <td>I think they're still good.</td>\n",
       "      <td>i think theyre still good</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>XyeZuYY2kPY</td>\n",
       "      <td>side note, this is a really good dish to sneak...</td>\n",
       "      <td>side note this is a really good dish to sneak ...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>301 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                  original_sentence  \\\n",
       "2    9RO83PiuQ-A  if you don't like cooking and washing dishes b...   \n",
       "673  05J3vTYNqCw  you have this yummy, delicious, chocolatey ras...   \n",
       "343  lZRU50mb0Yg  also had leftover veggies and this delicious s...   \n",
       "343  lZRU50mb0Yg                       it is delicious and filling.   \n",
       "606  uWBfiPSmU3s  our bodies need carbs to function optimally an...   \n",
       "..           ...                                                ...   \n",
       "367  0LCXoyHJJQ4  we practice eating the amount that makes our b...   \n",
       "368  Pjq7JEHFS5A           yeah, but you'd like never know so good.   \n",
       "371  VIHn1gCdQNk                                    oh so, so good.   \n",
       "372  -6nB_9dGRZc                        I think they're still good.   \n",
       "718  XyeZuYY2kPY  side note, this is a really good dish to sneak...   \n",
       "\n",
       "                                    tokenized_sentence  sentiment_score  \\\n",
       "2    if you dont like cooking and washing dish but ...              1.0   \n",
       "673  you have this yummy delicious chocolatey raspb...              1.0   \n",
       "343  also had leftover veggie and this delicious se...              1.0   \n",
       "343                        it is delicious and filling              1.0   \n",
       "606  our body need carbs to function optimally and ...              1.0   \n",
       "..                                                 ...              ...   \n",
       "367  we practice eating the amount that make our bo...              0.7   \n",
       "368              yeah but youd like never know so good              0.7   \n",
       "371                                      oh so so good              0.7   \n",
       "372                          i think theyre still good              0.7   \n",
       "718  side note this is a really good dish to sneak ...              0.7   \n",
       "\n",
       "     is_informative  \n",
       "2                 1  \n",
       "673               1  \n",
       "343               1  \n",
       "343               1  \n",
       "606               1  \n",
       "..              ...  \n",
       "367               1  \n",
       "368               1  \n",
       "371               1  \n",
       "372               1  \n",
       "718               1  \n",
       "\n",
       "[301 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_sentiment_threshold = 0.7\n",
    "# Filter informative n-grams with high sentiment scores\n",
    "informative_high_sentiment = infotranscripts_df[infotranscripts_df['sentiment_score'] >= high_sentiment_threshold]\n",
    "# Filter misinformative n-grams with high sentiment scores\n",
    "misinformative_high_sentiment = mistranscripts_df[mistranscripts_df['sentiment_score'] >= high_sentiment_threshold]\n",
    "\n",
    "informative_high_sentiment_sorted = informative_high_sentiment.sort_values(by='sentiment_score', ascending=False)\n",
    "misinformative_high_sentiment_sorted = misinformative_high_sentiment.sort_values(by='sentiment_score', ascending=False)\n",
    "informative_high_sentiment_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "informative_high_sentiment_sorted.to_csv('info_positive_sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>original_sentence</th>\n",
       "      <th>tokenized_sentence</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>is_informative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>EywXwM-pBYE</td>\n",
       "      <td>there you have it, the best saturated fats to ...</td>\n",
       "      <td>there you have it the best saturated fat to eat</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>twhrWKjRKi0</td>\n",
       "      <td>so there you guys have it: the best polyunsatu...</td>\n",
       "      <td>so there you guy have it the best polyunsatura...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>BX7g_cudbAM</td>\n",
       "      <td>plants are the best thing on the earth.</td>\n",
       "      <td>plant are the best thing on the earth</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>0IpYMo2m1A0</td>\n",
       "      <td>they're all delicious.</td>\n",
       "      <td>theyre all delicious</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>d8hLqTmlwAQ</td>\n",
       "      <td>so there you have it: the best monounsaturated...</td>\n",
       "      <td>so there you have it the best monounsaturated ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>YswrOr0A4gQ</td>\n",
       "      <td>I'm going to be good because I want to go to M...</td>\n",
       "      <td>im going to be good because i want to go to mi...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>W4tfE5H6Ong</td>\n",
       "      <td>getting good quality dairy in your life will i...</td>\n",
       "      <td>getting good quality dairy in your life will i...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>3kge5IpJ7LQ</td>\n",
       "      <td>but it's good.</td>\n",
       "      <td>but it good</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>3kge5IpJ7LQ</td>\n",
       "      <td>it's so good for you.</td>\n",
       "      <td>it so good for you</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>c956Rmjfquw</td>\n",
       "      <td>they understand that time is a valuable resour...</td>\n",
       "      <td>they understand that time is a valuable resour...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>318 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID                                  original_sentence  \\\n",
       "488   EywXwM-pBYE  there you have it, the best saturated fats to ...   \n",
       "437   twhrWKjRKi0  so there you guys have it: the best polyunsatu...   \n",
       "520   BX7g_cudbAM            plants are the best thing on the earth.   \n",
       "490   0IpYMo2m1A0                             they're all delicious.   \n",
       "489   d8hLqTmlwAQ  so there you have it: the best monounsaturated...   \n",
       "...           ...                                                ...   \n",
       "755   YswrOr0A4gQ  I'm going to be good because I want to go to M...   \n",
       "782   W4tfE5H6Ong  getting good quality dairy in your life will i...   \n",
       "789   3kge5IpJ7LQ                                     but it's good.   \n",
       "789   3kge5IpJ7LQ                              it's so good for you.   \n",
       "1256  c956Rmjfquw  they understand that time is a valuable resour...   \n",
       "\n",
       "                                     tokenized_sentence  sentiment_score  \\\n",
       "488     there you have it the best saturated fat to eat              1.0   \n",
       "437   so there you guy have it the best polyunsatura...              1.0   \n",
       "520               plant are the best thing on the earth              1.0   \n",
       "490                                theyre all delicious              1.0   \n",
       "489   so there you have it the best monounsaturated ...              1.0   \n",
       "...                                                 ...              ...   \n",
       "755   im going to be good because i want to go to mi...              0.7   \n",
       "782   getting good quality dairy in your life will i...              0.7   \n",
       "789                                         but it good              0.7   \n",
       "789                                  it so good for you              0.7   \n",
       "1256  they understand that time is a valuable resour...              0.7   \n",
       "\n",
       "      is_informative  \n",
       "488                0  \n",
       "437                0  \n",
       "520                0  \n",
       "490                0  \n",
       "489                0  \n",
       "...              ...  \n",
       "755                0  \n",
       "782                0  \n",
       "789                0  \n",
       "789                0  \n",
       "1256               0  \n",
       "\n",
       "[318 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misinformative_high_sentiment_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "misinformative_high_sentiment_sorted.to_csv('mis_positive_sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['if you dont like cooking and washing dish but still want a delicious meal then make a one pot meal in your rice cooker',\n",
       " 'you have this yummy delicious chocolatey raspberry treat',\n",
       " 'also had leftover veggie and this delicious seasoning',\n",
       " 'it is delicious and filling',\n",
       " 'our body need carbs to function optimally and feel their best',\n",
       " 'this is not a shamrock shake but it is a minty smoothie and it is delicious',\n",
       " 'costco costcos the best',\n",
       " 'mel health babore xoxo say it is like the best volum meting hack',\n",
       " 'pizza soup that you got to make asap a because it is insanely delicious but b time for soup season is running out friend',\n",
       " 'also delicious it the spot']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_informative_sentences = informative_high_sentiment_sorted.head(10)['tokenized_sentence'].tolist()\n",
    "top_informative_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['there you have it the best saturated fat to eat',\n",
       " 'so there you guy have it the best polyunsaturated fat to look for',\n",
       " 'plant are the best thing on the earth',\n",
       " 'theyre all delicious',\n",
       " 'so there you have it the best monounsaturated fat to eat',\n",
       " 'so there you guy have it the best polyunsaturated fat to look for',\n",
       " 'im going to show you the best animalbased food here',\n",
       " 'tomato help to regulate seum production reduce inflammation and promote skin healing making them an excellent addition to your antiacne diet',\n",
       " 'hey you guy know egg are both delicious and nutritious',\n",
       " 'thats always the best answer']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_misinformative_sentences = misinformative_high_sentiment_sorted.head(10)['tokenized_sentence'].tolist()\n",
    "top_misinformative_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sentiment_threshold = -0.3\n",
    "\n",
    "informative_low_sentiment = infotranscripts_df[infotranscripts_df['sentiment_score'] <= negative_sentiment_threshold]\n",
    "misinformative_low_sentiment = mistranscripts_df[mistranscripts_df['sentiment_score'] <= negative_sentiment_threshold]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "informative_low_sentiment_sorted = informative_low_sentiment.sort_values(by='sentiment_score', ascending=True)\n",
    "misinformative_low_sentiment_sorted = misinformative_low_sentiment.sort_values(by='sentiment_score', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "informative_low_sentiment_sorted.to_csv('info_negative_sentences.csv')\n",
    "misinformative_low_sentiment_sorted.to_csv('mis_negative_sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"i won't make you restrict anything and i won't make your life miserable.\",\n",
       " 'so I substituted the spinach for leaves that I found in my backyard and it tasted terrible.',\n",
       " \"soy sauce is one of the worst soy sauces i've ever had.\",\n",
       " \"let's talk the shocking truth about long-term ozempicues, according to an interview with the creator of glp-1, Agnes medications, Yen's host.\",\n",
       " 'I feel ya tuna salad s are boring.',\n",
       " 'oh, why did I eat that I feel so disgusting like?',\n",
       " \"you're the worst.\",\n",
       " 'speaking of tuna, I used to hyperfixate on tuna sandwiches from Subway as a kid and I would always get the worst stomach aches after eating them.',\n",
       " \"I'm miserable and spend all day thinking about food on this diet.\",\n",
       " \"when that happens you're feeling real bad about yourself.\"]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worst_informative_sentences = informative_low_sentiment_sorted.head(10)['original_sentence'].tolist()\n",
    "worst_misinformative_sentences = misinformative_low_sentiment_sorted.head(10)['original_sentence'].tolist()\n",
    "worst_informative_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is just about the worst thing anyone could drink to lose weight.',\n",
       " 'these are both horrible options.',\n",
       " 'it was nasty.',\n",
       " 'this oil is the worst oil you can ever consume.',\n",
       " 'this is insane, guys.',\n",
       " 'the real bad guy are seed oils.',\n",
       " \"or, the worst case, now I've given it to people.\",\n",
       " 'horrible breakfast.',\n",
       " 'I found I stumbled across this doctor called Dr sha Baker and, um, I thought this guy is absolutely insane.',\n",
       " \"they're not the worst when it comes to pesticides and you're not eating the skin or bananas where you can peel them.\"]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worst_misinformative_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on Whole transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model to predict whether a video is misinformative or not\n",
    "\n",
    "I built a document level sentiment analysis using transformer models using the peak end theory to account for the psychology of how a user reads and remembers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RobertaTokenizer\n",
    "## Readability index\n",
    "# CNN\n",
    "# XGBoost\n",
    "# TF-IDF vectorization\n",
    "\n",
    "# NER: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
