{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rileybaerg/TruthTube/blob/main/TruthTubePracticumColab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCoM7T-guu3q"
      },
      "source": [
        "# TruthTube Project Practicum\n",
        "Riley, Jiyoon, Keyan, Mustafa\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ6T9ZBQXMM9"
      },
      "source": [
        "## Problem\n",
        "\n",
        "We are concerned with the spread of health misinformation on YouTube, especially within YouTube shorts where the duration of a video is less than 60 seconds and spreads at a fast pace. We wish to examine how misinformative videos influence viewers, particularly focusing on the topic of Nutrition.\n",
        "\n",
        "RQ: How does the interaction between the content of a nutritional health video impact its popularity and uptake of content by viewers when it is informative versus misinformative?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxeIfZzdm_oB"
      },
      "source": [
        "## Method\n",
        "\n",
        "We decide to analyze comments from YouTube Shorts videos surrounding nutritional health topics. Text data from comments will help us understand the interactions between a video and its viewers. We select misinformative health channels and informative health channels. We select 3 YouTube channels for each category, processing data from a total of 6 channels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwE7kr8hWAHu"
      },
      "source": [
        "### Criteria\n",
        "In order to define informative health content, we follow\n",
        "https://www.ncbi.nlm.nih.gov/books/NBK225306/, where they indicate that \"the registered dietitian is the single identifiable group of healthcare professionals with the standardized education, clinical training, continuing education, and national credentialing requirements necessary to provide nutrition therapy.\"\n",
        "\n",
        "We noticed that the channel owner YouTube Shorts regarding nutritional health. Therefore, we set the criteria of a channel owner as having the qualifications of:\n",
        "\n",
        "* A Registered Dietitian (RD or RDN)\n",
        "* A person whose claims doesn't go against current nutrition guidelines provided by websites listed in https://openoregon.pressbooks.pub/nutritionscience/chapter/2e-who-can-you-trust/\n",
        "* Someone who does not push an agenda or product available after purpose (those leveraging their health education into a health-business related career); promoting self-interests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO14Wfl7WjOX"
      },
      "source": [
        "## Quantiative\n",
        "### Channel Information and Uploads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uuH8EJVOKNL3"
      },
      "outputs": [],
      "source": [
        "from googleapiclient.discovery import build\n",
        "import pandas as pd\n",
        "\n",
        "api_key = \"AIzaSyBHMgFLFQX2mvLQqD4do9xrTv07hjPPX0U\"\n",
        "api_service_name = \"youtube\"\n",
        "api_version = \"v3\"\n",
        "youtube = build(\n",
        "    api_service_name, api_version, developerKey=api_key\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vsf41l3_iTul"
      },
      "source": [
        "Using the YouTube Data API, we first collect summary information for YouTube channels. We use the YouTube channel's ID to identify each YouTube channel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "HOlrFnmGWi2V"
      },
      "outputs": [],
      "source": [
        "def get_channel_info(userid):\n",
        "    request = youtube.channels().list(\n",
        "        part=\"snippet,contentDetails,statistics\",\n",
        "        id=userid\n",
        "    )\n",
        "    response = request.execute()\n",
        "    item = response['items'][0]\n",
        "    # Your solution\n",
        "    return {\n",
        "        'channelName': item['snippet']['title'],\n",
        "        'channelStartDate': item['snippet']['publishedAt'],\n",
        "        'subscribers': item['statistics']['subscriberCount'],\n",
        "        'viewCount': item['statistics']['viewCount'],\n",
        "        'videoCount': item['statistics']['videoCount'],\n",
        "        'uploadsPlaylist': item['contentDetails']['relatedPlaylists']['uploads']\n",
        "    }\n",
        "def get_video_data(video_ids):\n",
        "    video_data = []\n",
        "    for i in range (0, len(video_ids), 50): # performs requests in batches to avoid rate-limiting\n",
        "        request = youtube.videos().list(\n",
        "            part='snippet,contentDetails,statistics',\n",
        "            id=','.join(video_ids[i:i+50])\n",
        "        )\n",
        "        response = request.execute() #record response\n",
        "\n",
        "        for item in response['items']:\n",
        "            relevant_stats = {\n",
        "                'snippet': ['title', 'description', 'tags', 'publishedAt'],\n",
        "                'statistics': ['viewCount', 'likeCount', 'commentCount'],\n",
        "                'contentDetails': ['duration', 'definition', 'caption']\n",
        "            } #collects information that we care about... check documentation to choose information\n",
        "\n",
        "            video_info = {}\n",
        "            video_info['video_id'] = item['id']\n",
        "\n",
        "            for cat in relevant_stats.keys():\n",
        "                for stat in relevant_stats[cat]:\n",
        "                    try:\n",
        "                        video_info[stat] = item[cat][stat]\n",
        "                    except:\n",
        "                        video_info[stat] = None\n",
        "\n",
        "            video_data.append(video_info)\n",
        "    video_data\n",
        "    return pd.DataFrame(video_data)\n",
        "\n",
        "# Define any helper functions here\n",
        "def get_video_ids(playlistID):\n",
        "    request = youtube.playlistItems().list(\n",
        "        part='snippet,contentDetails',\n",
        "        playlistId = playlistID,\n",
        "        maxResults=50\n",
        "    )\n",
        "\n",
        "    response = request.execute()\n",
        "\n",
        "    video_ids = []\n",
        "    video_ids.extend([item['contentDetails']['videoId'] for item in response['items']])\n",
        "\n",
        "    next_page = response.get('nextPageToken')\n",
        "\n",
        "    while next_page is not None:\n",
        "        request = youtube.playlistItems().list(\n",
        "            part='snippet,contentDetails',\n",
        "            playlistId = playlistID,\n",
        "            maxResults = 50,\n",
        "            pageToken = next_page\n",
        "        )\n",
        "        response = request.execute()\n",
        "\n",
        "        video_ids.extend([item['contentDetails']['videoId'] for item in response['items']])\n",
        "        next_page = response.get('nextPageToken')\n",
        "\n",
        "    return video_ids\n",
        "\n",
        "def get_channel_data(userid):\n",
        "    channel_info = get_channel_info(userid)\n",
        "    shorts_playlistid = \"UUSH\" + userid[2:]\n",
        "\n",
        "    video_ids = get_video_ids(shorts_playlistid)\n",
        "    upload_data = get_video_data(video_ids)\n",
        "    upload_data.insert(0, 'channelName', channel_info['channelName'])\n",
        "\n",
        "    return channel_info, upload_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n07RpCcxmOoW"
      },
      "source": [
        "Based on the criteria, we identify misinformative and informative youtube channels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "goLPwOlXKhEQ"
      },
      "outputs": [],
      "source": [
        "# Misinformative youtube channels\n",
        "mis1_id = \"UC3w193M5tYPJqF0Hi-7U-2g\"\n",
        "mis2_id = \"UC5apkKkeZQXRSDbqSalG8CQ\"\n",
        "mis3_id = \"UCgBg0LcHfnJDPmFTTf677Pw\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "jt7uQFmnKxyl"
      },
      "outputs": [],
      "source": [
        "# Informative channel ids\n",
        "inf1_id = \"UCcffZfMDLakH-hs89uSKxQg\"\n",
        "inf2_id = \"UCKLz-9xkpPNjK26PqbjHn7Q\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zH-F_5T8rCVw"
      },
      "source": [
        "Then we pull data about these channels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "POEF-FklIcIv"
      },
      "outputs": [],
      "source": [
        "mis1_info, mis1_uploads = get_channel_data(mis1_id)\n",
        "mis2_info, mis2_uploads = get_channel_data(mis2_id)\n",
        "mis3_info, mis3_uploads = get_channel_data(mis3_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "7HXVpTe0K576"
      },
      "outputs": [],
      "source": [
        "inf1_info, inf1_uploads = get_channel_data(inf1_id)\n",
        "inf2_info, inf2_uploads = get_channel_data(inf2_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgOmxoraSYrl"
      },
      "source": [
        "We add boolean values to each group, adding a column named is_informative. (informative videos: 1, misinformative videos: 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "GDBv8bEMSYLj"
      },
      "outputs": [],
      "source": [
        "mis1_uploads['is_informative'] = int(False)\n",
        "mis2_uploads['is_informative'] = int(False)\n",
        "mis3_uploads['is_informative'] = int(False)\n",
        "inf1_uploads['is_informative'] = int(True)\n",
        "inf2_uploads['is_informative'] = int(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iFRfYp0TBRu"
      },
      "source": [
        "We have 3 dataframes that contain upload information for each channel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Wa3J_JooS4c5"
      },
      "outputs": [],
      "source": [
        "misinfo_df = pd.concat([mis1_uploads, mis2_uploads, mis3_uploads], axis=0)\n",
        "info_df = pd.concat([inf1_uploads, inf2_uploads], axis=0)\n",
        "youtube_df = pd.concat([misinfo_df, info_df], axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EEN_-A3x_0R"
      },
      "source": [
        "#### Analyzing channel general information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm0UMkev2oo7"
      },
      "source": [
        "Calculate the average view count, like cout, and comment count of all channels to have a general idea of the size of the channel and its interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "GW6vy8U91Qu8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "channelName       object\n",
            "video_id          object\n",
            "title             object\n",
            "description       object\n",
            "tags              object\n",
            "publishedAt       object\n",
            "viewCount         object\n",
            "likeCount         object\n",
            "commentCount      object\n",
            "duration          object\n",
            "definition        object\n",
            "caption           object\n",
            "is_informative     int64\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "#check data type of the columns analyzing\n",
        "print(youtube_df.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "gT2ngxv80n7b"
      },
      "outputs": [],
      "source": [
        "#convert data type from object to integer for analysis\n",
        "youtube_df['viewCount'] = pd.to_numeric(youtube_df['viewCount'], errors='coerce', downcast='integer')\n",
        "youtube_df['commentCount'] = pd.to_numeric(youtube_df['commentCount'], errors='coerce', downcast='integer')\n",
        "youtube_df['likeCount'] = pd.to_numeric(youtube_df['likeCount'], errors='coerce', downcast='integer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "EqiqAIDox-yk",
        "outputId": "825dbd1b-f739-49b7-a27e-453b14644e69"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Channel Name</th>\n",
              "      <th>Average View Count</th>\n",
              "      <th>Average Comment Count</th>\n",
              "      <th>Average Like Count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Abbey Sharp</td>\n",
              "      <td>243457.11</td>\n",
              "      <td>290.50</td>\n",
              "      <td>12350.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dr. Eric Berg DC</td>\n",
              "      <td>247269.38</td>\n",
              "      <td>400.86</td>\n",
              "      <td>14232.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Nutrition By Kylie</td>\n",
              "      <td>1989573.26</td>\n",
              "      <td>589.78</td>\n",
              "      <td>125964.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Paul Saladino MD</td>\n",
              "      <td>245392.89</td>\n",
              "      <td>604.73</td>\n",
              "      <td>9945.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Shawn Baker MD</td>\n",
              "      <td>31796.15</td>\n",
              "      <td>109.66</td>\n",
              "      <td>1956.15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Channel Name  Average View Count  Average Comment Count  \\\n",
              "0         Abbey Sharp           243457.11                 290.50   \n",
              "1    Dr. Eric Berg DC           247269.38                 400.86   \n",
              "2  Nutrition By Kylie          1989573.26                 589.78   \n",
              "3    Paul Saladino MD           245392.89                 604.73   \n",
              "4      Shawn Baker MD            31796.15                 109.66   \n",
              "\n",
              "   Average Like Count  \n",
              "0            12350.39  \n",
              "1            14232.49  \n",
              "2           125964.16  \n",
              "3             9945.75  \n",
              "4             1956.15  "
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calculate average view count, comment count, and like count for each channel\n",
        "average_metrics = youtube_df.groupby('channelName').agg({\n",
        "    'viewCount': 'mean',\n",
        "    'commentCount': 'mean',\n",
        "    'likeCount': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "# Rename columns for clarity\n",
        "average_metrics.columns = ['Channel Name', 'Average View Count', 'Average Comment Count', 'Average Like Count']\n",
        "\n",
        "#change the avaerge number format to decimal\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "\n",
        "# Display the table with average metrics for each channel\n",
        "average_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO-KrIY9Mem1"
      },
      "source": [
        "### Retrieving Comments Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMSVTr81vDjm"
      },
      "source": [
        "####Data Cleaning and processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiRwgIzEIjI7"
      },
      "source": [
        "Processing data using pandas\n",
        "(Code below uses arbitrary names and names need to be replaced accordingly when using)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "_JSXHAt5rHA7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "pd.options.display.max_rows = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "9I4uskwVJFLB"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'name.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m name_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#replace name.csv with actual name of the csv file, relace name_df with a good naming for this df\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'name.csv'"
          ]
        }
      ],
      "source": [
        "name_df = pd.read_csv('name.csv', delimiter=\",\") #replace name.csv with actual name of the csv file, relace name_df with a good naming for this df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PalcRkgvJbx7"
      },
      "source": [
        "Change all comments to lower cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LRRfJPlJhdD"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'comments'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mKeyError\u001b[0m: 'comments'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mname_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcomments\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;66;03m#if the name of the column comments are in is different, address accoridngly\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[1;31mKeyError\u001b[0m: 'comments'"
          ]
        }
      ],
      "source": [
        "name_df['comments'].str.lower() #if the name of the column comments are in is different, address accoridngly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEUg3ls7JnDe"
      },
      "source": [
        "Remove special characters and keep alphanumeric characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5uLBC1WJtjV"
      },
      "outputs": [],
      "source": [
        "def remove_special_characters(text):\n",
        "    pattern = r'[^a-zA-Z0-9\\s]'  # Keep alphanumeric characters and whitespace\n",
        "    return re.sub(pattern, '', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSi6st2bNiJ5"
      },
      "outputs": [],
      "source": [
        "# Apply the function to the 'comments' column, store cleaned comments in 'clean_comments'\n",
        "name_df['clean_comments'] = name_df['comments'].apply(remove_special_characters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmovhnfBJ4Jd"
      },
      "outputs": [],
      "source": [
        "name_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbU8AFfpJ8Mw"
      },
      "source": [
        "Split comments into token using nltk library.(Here I'm spliting by word but we also have options to do sentence token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjETSrckJ7E9"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIVQWkVyNu3d"
      },
      "outputs": [],
      "source": [
        "def tokenize_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzVZz9DjNyVw"
      },
      "outputs": [],
      "source": [
        "# Apply the function to the 'clean_comments' column, store tokenized comments in 'tokenized_comments'\n",
        "name_df['tokenized_comments']=name_df['clean_comments'].apply(tokenize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-5DInBgN1iw"
      },
      "outputs": [],
      "source": [
        "name_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoKhESfoFtPq"
      },
      "source": [
        "Removing stop words using NLTK library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4TmP7DnFs1a",
        "outputId": "06799b8a-87d5-4168-cf03-2d7252eb7e9b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-F1r6XXXGZIo"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(tokens):\n",
        "  # Get the set of English stopwords from NLTK\n",
        "    english_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "    # Filter out stopwords from the list of tokens\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in english_stopwords]\n",
        "\n",
        "    return filtered_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIgF5K7XG3a0"
      },
      "outputs": [],
      "source": [
        "# Apply remove_stopwords function to the 'tokenized_words' column\n",
        "name_df['filtered_words'] = name_df['tokenized_words'].apply(remove_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Condensed Comment Preprocessing Methods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\baerg\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\baerg\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'comments'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mKeyError\u001b[0m: 'comments'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[38], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# apply function to comment dataframe (name for now, until official variable name is established)\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m name_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclean_comments\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mname_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcomments\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mapply(clean_tokenize_text)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[1;31mKeyError\u001b[0m: 'comments'"
          ]
        }
      ],
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "# download Natural Language Toolkit parameter learners and stopword removal resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# encompassing text cleaning and tokenizing function\n",
        "def clean_tokenize_text(text):\n",
        "  # convert text to all lowercase\n",
        "  text = text.lower()\n",
        "  # remove any special characters and ensure that only alphanumeric characters are kept\n",
        "  text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "  # tokenize the text\n",
        "  tokens = word_tokenize(text)\n",
        "  # stopword removal steps\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [token for token in tokens if token not in stop_words]\n",
        "  return tokens\n",
        "\n",
        "# apply function to comment dataframe (name for now, until official variable name is established)\n",
        "name_df['clean_comments'] = name_df['comments'].str.lower().apply(clean_tokenize_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analysis and Visualization of Comments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'comments'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mKeyError\u001b[0m: 'comments'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[34], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m blob\u001b[38;5;241m.\u001b[39msentiment\u001b[38;5;241m.\u001b[39mpolarity\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Apply the sentiment analysis function to current data frame\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m name_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mname_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcomments\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(calculate_sentiment_score)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Visualize the sentiment distribution using matplotlib\u001b[39;00m\n\u001b[0;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(name_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_score\u001b[39m\u001b[38;5;124m'\u001b[39m], bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskyblue\u001b[39m\u001b[38;5;124m'\u001b[39m, edgecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[1;31mKeyError\u001b[0m: 'comments'"
          ]
        }
      ],
      "source": [
        "from textblob import TextBlob #!pip install textblob ;)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# sentiment score calcuation function utilizing textblob\n",
        "def calculate_sentiment_score(text):\n",
        "  blob = TextBlob(text)\n",
        "  return blob.sentiment.polarity\n",
        "\n",
        "# Apply the sentiment analysis function to current data frame\n",
        "name_df['sentiment_score'] = name_df['comments'].apply(calculate_sentiment_score)\n",
        "\n",
        "# Visualize the sentiment distribution using matplotlib\n",
        "plt.hist(name_df['sentiment_score'], bins=20, color='skyblue', edgecolor='black')\n",
        "plt.title('Sentiment Distribution of Comments')\n",
        "plt.xlabel('Sentiment Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkYAsf6pKUnG"
      },
      "source": [
        "##Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq9Dy8NdWmqZ"
      },
      "source": [
        "## Credits\n",
        "Credit listing of what each team member contributed to completing this part of the project.\n",
        "\n",
        "Riley\n",
        "*   Attended project check-ins and meetings to refine research question.\n",
        "*   Outlined groups Data Science process in attempt to solidify each steps contribution to addressing our research question.\n",
        "*   Contributed to writing YouTube Comment data analysis and visualization (calculate_sentiment_score)\n",
        "*   Assessed and contributed to YouTube Comment Preprocessing code (clean_tokenize_text)\n",
        "\n",
        "Jiyoon\n",
        "*   Attended project check-ins and meetings to refine research question\n",
        "*   Wrote problem, method, criteria sections\n",
        "*   Wrote code for receiving channel data\n",
        "*   Wrote code for creating uploads data (misinfo_df, info_df, youtube_df)\n",
        "\n",
        "Keyan\n",
        "*   Attended project check-ins and meetings to refine research question\n",
        "*   Wrote code for data cleaning and processing part\n",
        "*   Wrote code for general channel analysis (computing averages)\n",
        "\n",
        "Mustafa\n",
        "*   List item\n",
        "*   List item"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMIEXpFJWyAI"
      },
      "source": [
        "## Processed data files\n",
        "Processed data files (in compressed form if large), preferably in CSV format. If it still does not fit, let us know"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSpn8TDrIqm7"
      },
      "outputs": [],
      "source": [
        "youtube_df.to_csv('all_uploads.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
